{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081b076f",
   "metadata": {},
   "source": [
    "<h2><font color=\"#004D7F\" size=6> Procesamiento de textos  Tarea4</font></h2>\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#004D7F\" size=3>Danyela Luengas</font><br>\n",
    "<font color=\"#004D7F\" size=3>Procesamiento de lenguaje natural o minería de textos</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae49b60",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los textos puede:\n",
    "\n",
    "1. Estandarizar el texto a minúsculas\n",
    "2. Eliminar las menciones a usuarios (@user)\n",
    "3. Eliminar las url’s\n",
    "4. Eliminar los emojis\n",
    "5. Las abreviaturas, contracciones y slangs sustituirlas por el texto equivalente\n",
    "6. Eliminar palabras funcionales\n",
    "7. Verificar si existen cifras numéricas, las cuales pueden ser reemplazadas por algún término o eliminarlas\n",
    "8. Tratamiento con los hashtags\n",
    "9. Eliminar caracteres raros y especiales\n",
    "10. Eliminar signos de puntuación\n",
    "11. Estandarizar las secuencias de varios espacios en blanco, tabuladores y saltos de línea\n",
    "Entre otras…\n",
    "Posibles características para tenerse en cuenta:\n",
    "\n",
    "1. N-gramas de caracteres\n",
    "2. N-gramas de palabras\n",
    "3. N-gramas de etiquetas POS\n",
    "4. N-gramas de saltos de palabras (skip-gram)\n",
    "5. N-gramas de palabras funcionales\n",
    "6. N-gramas de símbolos de puntuación\n",
    "7. Entre otras…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad686227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f4f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=pd.read_csv('train_es2.tsv',delimiter='\\t', encoding='utf8')\n",
    "\n",
    "des=pd.read_csv('dev_es2.tsv',delimiter='\\t', encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44c6a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return file\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()   #  convertir en minuscula\n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    \n",
    "    text= remove_emoji(text)\n",
    "    text= remove_stopwords(text)\n",
    "    text=re.sub(\"\\d+\", \"0\", text)\n",
    "    # text=re.sub(\"\\d+\", \" \", text)\n",
    "    \n",
    "    #elimnar SIGNOS DE PUNTUACIÓN\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "#\n",
    "def remove_stopwords(text):    \n",
    "    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d156422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23fff0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'hs', 'tr', 'ag', 'easyjet', 'quiere', 'duplicar', 'el', 'número', 'de', 'mujeres', 'piloto', 'verás', 'tú', 'para', 'aparcar', 'el', 'avión', 'http', 'el', 'gobierno', 'debe', 'crear', 'un', 'control', 'estricto', 'de', 'inmigración', 'en', 'las', 'zonas', 'fronterizas', 'con', 'colombia', 'por', 'q', 'después', 'del', 'venir', 'en', 'masa', 'yo', 'veo', 'mujeres', 'destruidas', 'por', 'acoso', 'laboral', 'callejero', 'otras', 'con', 'depresión', 'debido', 'violación', 'sexual', 'maltrato', 'físico', 'conocí', 'varias', 'que', 'se', 'suicidaron', 'por', 'este', 'tipo', 'de', 'comportamientos', 'machistas', 'vas', 'seguir', 'con', 'tu', 'show', 'pobre', 'https', 'yo', 'soy', 'respetuoso', 'con', 'los', 'demás', 'sólamente', 'les', 'recuerdo', 'lo', 'que', 'son', 'si', 'son', 'escoria', 'es', 'culpa', 'mía', 'que', 'lo', 'sé', 'quién', 'tomas', 'antonio']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed300246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20005</td>\n",
       "      <td>comiendo picada árabe rica vida</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20006</td>\n",
       "      <td>callate zorra mama duro</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20011</td>\n",
       "      <td>acabo escuchar casado diciendo decenas millone...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20019</td>\n",
       "      <td>nadie preguntado va costar hombre guantazo sit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20033</td>\n",
       "      <td>callate come sobra zorra</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>24930</td>\n",
       "      <td>habe pasame video adan diga puta mujeres sevil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>24953</td>\n",
       "      <td>callate hijo perra amor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>24973</td>\n",
       "      <td>auténtico imbécil potencial machista ideología...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>24983</td>\n",
       "      <td>callate q paso mili m paso ami faltas vos gord...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>24986</td>\n",
       "      <td>traducción escucha puta sueca árabes aquí apod...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  HS  TR  AG\n",
       "0    20005                    comiendo picada árabe rica vida   0   0   0\n",
       "1    20006                            callate zorra mama duro   1   1   1\n",
       "2    20011  acabo escuchar casado diciendo decenas millone...   0   0   0\n",
       "3    20019  nadie preguntado va costar hombre guantazo sit...   1   0   1\n",
       "4    20033                           callate come sobra zorra   1   1   1\n",
       "..     ...                                                ...  ..  ..  ..\n",
       "495  24930  habe pasame video adan diga puta mujeres sevil...   1   0   1\n",
       "496  24953                            callate hijo perra amor   0   0   0\n",
       "497  24973  auténtico imbécil potencial machista ideología...   0   0   0\n",
       "498  24983  callate q paso mili m paso ami faltas vos gord...   1   1   1\n",
       "499  24986  traducción escucha puta sueca árabes aquí apod...   1   1   1\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train=procesar(des, 'train_es_clean2.tsv')\n",
    "dev=procesar(tr, 'dev_es_clean2.tsv')\n",
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55e235db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      20005\n",
       "1      20006\n",
       "2      20011\n",
       "3      20019\n",
       "4      20033\n",
       "       ...  \n",
       "495    24930\n",
       "496    24953\n",
       "497    24973\n",
       "498    24983\n",
       "499    24986\n",
       "Name: id, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Train[Train.columns[0]]\n",
    "X_train_textA = Train[Train.columns[1]].fillna(' ')\n",
    "y_train_hsA = Train[Train.columns[2]]\n",
    "\n",
    "test = dev[dev.columns[0]]\n",
    "X_test_textA = dev[dev.columns[1]].fillna(' ')\n",
    "y_test_hsA = dev[dev.columns[2]]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac5a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "txt = \"Company10\"\n",
    "\n",
    "x = txt.isalpha()\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45a1897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n",
      "4469 4469\n"
     ]
    }
   ],
   "source": [
    "print( len(X_train_textA), len(y_train_hsA) )\n",
    "\n",
    "print( len(X_test_textA), len(y_test_hsA) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "087e2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 152)\n",
      "(4469, 152)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "495    1\n",
       "496    0\n",
       "497    0\n",
       "498    1\n",
       "499    1\n",
       "Name: HS, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvectorizer = CountVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(1,3),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=5,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_cvectorized = cvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_cvectorized.shape)\n",
    "\n",
    "X_test_cvectorized = cvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_cvectorized.shape)\n",
    "y_train_hsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "348cefb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 152)\n",
      "(4469, 152)\n"
     ]
    }
   ],
   "source": [
    "tvectorizer = TfidfVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(1,3),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=5,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_tvectorized = tvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_tvectorized.shape)\n",
    "\n",
    "X_test_tvectorized = tvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_tvectorized.shape)\n",
    "\n",
    "\n",
    "y_test_hsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff053718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train_cvectorized)\n",
    "X_test = scaler.fit_transform(X_test_cvectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08551d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de LOR: 71.80% (std:6.60%)\n",
      "Accuracy de LDA: 64.80% (std:6.34%)\n",
      "Accuracy de k-NN: 69.60% (std:5.78%)\n",
      "Accuracy de CART: 64.60% (std:4.20%)\n",
      "Accuracy de NB: 67.00% (std:7.96%)\n",
      "Accuracy de GBC: 71.00% (std:6.08%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report, roc_curve\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Compare Algorithms\n",
    "# prepare models\n",
    "models=[]\n",
    "RMSE = []\n",
    "R_sq = []\n",
    "models.append(('LOR', LogisticRegression(solver='lbfgs', max_iter=1000)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('k-NN', KNeighborsClassifier(n_neighbors=3)))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('GBC',  GradientBoostingClassifier(n_estimators=200, learning_rate=0.7,\n",
    "                                 max_depth=1)))\n",
    "\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring= 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold= KFold(n_splits=10)\n",
    "    cv_results= cross_val_score(model,X_train, y_train_hsA, cv=kfold, scoring=scoring)\n",
    "    names.append(name)\n",
    "    print(f'Accuracy de {name}: {cv_results.mean()*100.0:,.2f}% (std:{cv_results.std()*100.0:,.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d0aa1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La probabilidad de acierto  para GradientBoostingClassifier \n",
      "\n",
      "67.03960617587828\n",
      "1067\n",
      "Matrix de Confusion \n",
      "\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          2280   351  2631\n",
      "1          1122   716  1838\n",
      "All        3402  1067  4469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76      2631\n",
      "           1       0.67      0.39      0.49      1838\n",
      "\n",
      "    accuracy                           0.67      4469\n",
      "   macro avg       0.67      0.63      0.62      4469\n",
      "weighted avg       0.67      0.67      0.65      4469\n",
      "\n",
      "Accuracy con Datos test 67.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.3612777509637537"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "GBC = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0,\n",
    "                                 max_depth=1)\n",
    "GBC.fit(X_train_tvectorized, y_train_hsA)\n",
    "predicGBC = GBC.predict(X_test)\n",
    "print('La probabilidad de acierto  para GradientBoostingClassifier \\n')\n",
    "print((np.sum(predicGBC == y_test_hsA)/len(predicGBC))*100)\n",
    "print(predicGBC.sum())\n",
    "print('Matrix de Confusion \\n')\n",
    "print(pd.crosstab(y_test_hsA, predicGBC, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "print(classification_report(y_test_hsA, predicGBC))\n",
    "\n",
    "print(\"Accuracy con Datos test {:.2f}\".format(accuracy_score(y_test_hsA, predicGBC)*100))\n",
    "#print(\"Accuracy con Datos validación {:.2f}\".format(accuracy_score(y_val, predicGBC)*100))\n",
    "r2 = r2_score(y_test_hsA, predicGBC)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b49c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la probabilidad de acierto  Naiver Bayes\n",
      "\n",
      "64.55582904452898\n",
      "2606\n",
      "matrix de Confusion\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          1455  1176  2631\n",
      "1           408  1430  1838\n",
      "All        1863  2606  4469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.55      0.65      2631\n",
      "           1       0.55      0.78      0.64      1838\n",
      "\n",
      "    accuracy                           0.65      4469\n",
      "   macro avg       0.66      0.67      0.65      4469\n",
      "weighted avg       0.69      0.65      0.65      4469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train_hsA)\n",
    "predicNB = NB.predict(X_test)\n",
    "\n",
    "print('la probabilidad de acierto  Naiver Bayes\\n')\n",
    "print((np.sum(predicNB == y_test_hsA)/len(predicNB))*100)\n",
    "print(predicNB.sum())\n",
    "print('matrix de Confusion')\n",
    "print(pd.crosstab(y_test_hsA, predicNB, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "print(classification_report(y_test_hsA, predicNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e242c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la probabilidad de acierto  Regresión logistica\n",
      "\n",
      "68.49407026180353\n",
      "1594\n",
      "matrix de Confusion\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          2049   582  2631\n",
      "1           826  1012  1838\n",
      "All        2875  1594  4469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74      2631\n",
      "           1       0.63      0.55      0.59      1838\n",
      "\n",
      "    accuracy                           0.68      4469\n",
      "   macro avg       0.67      0.66      0.67      4469\n",
      "weighted avg       0.68      0.68      0.68      4469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daalu\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train, y_train_hsA)\n",
    "prediclog= logreg.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test_hsA, prediclog)\n",
    "print('la probabilidad de acierto  Regresión logistica\\n')\n",
    "print((np.sum(prediclog == y_test_hsA)/len(prediclog))*100)\n",
    "print(prediclog.sum())\n",
    "print('matrix de Confusion')\n",
    "print(pd.crosstab(y_test_hsA, prediclog, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "print(classification_report(y_test_hsA, prediclog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5014a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la probabilidad de acierto Máquina de vectores de soporte Lineal\n",
      "\n",
      "71.44775117475946\n",
      "1466\n",
      "matrix de Confusion\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          2179   452  2631\n",
      "1           824  1014  1838\n",
      "All        3003  1466  4469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.77      2631\n",
      "           1       0.69      0.55      0.61      1838\n",
      "\n",
      "    accuracy                           0.71      4469\n",
      "   macro avg       0.71      0.69      0.69      4469\n",
      "weighted avg       0.71      0.71      0.71      4469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC(kernel=\"linear\")\n",
    "SVM.fit(X_train,y_train_hsA)\n",
    "\n",
    "predicSVM = SVM.predict(X_test)\n",
    "#print('combinacion',pair)\n",
    "print('la probabilidad de acierto Máquina de vectores de soporte Lineal\\n')\n",
    "print((np.sum(predicSVM == y_test_hsA)/len(predicSVM))*100)\n",
    "print(predicSVM.sum())\n",
    "print('matrix de Confusion')\n",
    "print(pd.crosstab(y_test_hsA, predicSVM, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "print(classification_report(y_test_hsA, predicSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b447571c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la probabilidad de acierto Máquina de vectores de soporte Lineal\n",
      "\n",
      "64.89147460281941\n",
      "423\n",
      "matrix de Confusion\n",
      "Predicted     0    1   All\n",
      "True                      \n",
      "0          2554   77  2631\n",
      "1          1492  346  1838\n",
      "All        4046  423  4469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.97      0.77      2631\n",
      "           1       0.82      0.19      0.31      1838\n",
      "\n",
      "    accuracy                           0.65      4469\n",
      "   macro avg       0.72      0.58      0.54      4469\n",
      "weighted avg       0.71      0.65      0.58      4469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC(degree=2, kernel=\"poly\",probability=True)\n",
    "SVM.fit(X_train,y_train_hsA)\n",
    "\n",
    "predicSVM = SVM.predict(X_test)\n",
    "#print('combinacion',pair)\n",
    "print('la probabilidad de acierto Máquina de vectores de soporte Lineal\\n')\n",
    "print((np.sum(predicSVM == y_test_hsA)/len(predicSVM))*100)\n",
    "print(predicSVM.sum())\n",
    "print('matrix de Confusion')\n",
    "print(pd.crosstab(y_test_hsA, predicSVM, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "print(classification_report(y_test_hsA, predicSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538a70f",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "El mejor algorimo de clasificación supervisada es el de Naiver Bayes Gaussian, dado que es el unico que no presenta un sobreajuste o uno sin ajuste, no obstante su métrica de presición o accuracy es bajo del $65\\%$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e13f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
