{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya instalamos paquetes NLTK usando nltk.download(). Uno de los paquetes fue WordNet.\n",
    "# Documentación: https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para conocer todos los synset de una palabra se usa\n",
    "syn = wn.synsets('dog')\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "['the dog barked all night']\n"
     ]
    }
   ],
   "source": [
    "# Obtener definiciones y ejemplos para una palabra (un synset en particular)\n",
    "print( wn.synset('dog.n.01').definition() )\n",
    "\n",
    "print( wn.synset('dog.n.01').examples() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the branch of information science that deals with natural language information\n",
      "large Old World boas\n"
     ]
    }
   ],
   "source": [
    "# Incluye muchas definiciones\n",
    "syn = wn.synsets(\"NLP\")\n",
    "print( syn[0].definition() )\n",
    "\n",
    "syn = wn.synsets(\"Python\")\n",
    "print( syn[0].definition() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> La función tiene un argumento <em>POS</em> opcional que le permite restringir la parte gramatical de la palabra. Las partes del discurso son: <code>NOUN, ADJ, VERB y ADV</code>. Un synset se identifica con un nombre de 3 partes de la forma: <code>word.pos.nn</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chase.v.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog', pos= wn.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.dog'),\n",
       " Lemma('dog.n.01.domestic_dog'),\n",
       " Lemma('dog.n.01.Canis_familiaris')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obteniendo los lemmas\n",
    "wn.synset('dog.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(lemma.name()) for lemma in wn.synset('dog.n.01').lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\daalu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['als',\n",
       " 'arb',\n",
       " 'bul',\n",
       " 'cat',\n",
       " 'cmn',\n",
       " 'dan',\n",
       " 'ell',\n",
       " 'eng',\n",
       " 'eus',\n",
       " 'fin',\n",
       " 'fra',\n",
       " 'glg',\n",
       " 'heb',\n",
       " 'hrv',\n",
       " 'ind',\n",
       " 'isl',\n",
       " 'ita',\n",
       " 'ita_iwn',\n",
       " 'jpn',\n",
       " 'lit',\n",
       " 'nld',\n",
       " 'nno',\n",
       " 'nob',\n",
       " 'pol',\n",
       " 'por',\n",
       " 'ron',\n",
       " 'slk',\n",
       " 'slv',\n",
       " 'spa',\n",
       " 'swe',\n",
       " 'tha',\n",
       " 'zsm']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw')\n",
    "\n",
    "# WordNet permiten acceso a otras lenguas\n",
    "sorted(wn.langs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'), Synset('decree.n.01'), Synset('mystery_play.n.01')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para hacer consultas sobre una palabra específica.\n",
    "wn.synsets(b'auto'.decode('utf-8'), lang='spa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto', 'automóvil', 'carro', 'coche', 'máquina', 'turismo', 'vehículo']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sobre el sysnset devuelto se pueden hacer consultas\n",
    "wn.synset('car.n.01').lemma_names('spa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.can'), Lemma('dog.n.01.perro')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wn.synset('dog.n.01').lemmas('spa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28647"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para saber cuántos lemmas de sustantivos en español tiene wordnet\n",
    "len( list( wn.all_lemma_names(pos='n', lang='spa') ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Synset:</b> conjunto de sinónimos que comparten un significado común.</li>\n",
    "    <li><b>Hiperónimos:</b> palabra cuyo significado engloba el de otra u otras. Es una manera más general de referirse a algo.</li>\n",
    "    <li><b>Hipónimos:</b> son palabras cuyo significado es específico y acotado. Es una forma específica de referirse a un concepto. Los hipónimos se pueden englobar en un hiperónimo, es decir, palabras que engloban un concepto más amplio.</li>\n",
    "    <li><b>Holónimo:</b> palabra cuyo significado mantiene, respecto del de otra, la misma relación que el todo respecto de la parte.</li>\n",
    "</ul>\n",
    "\n",
    "Ejemplos:\n",
    "<ul>\n",
    "    <li>flor (hiperónimo), rosa, jazmín, tulipán (hipónimos)</li>\n",
    "    <li>deporte (hiperónimo), fútbol, tenis, montañismo, natación (hipónimos)</li>\n",
    "    <li>Flor es el holónimo de cáliz, corola, pistilo o estambre</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hiperónimos\n",
    "dog = wn.synset('dog.n.01')\n",
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hipónimos\n",
    "dog.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canis.n.01'), Synset('pack.n.06')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Holónimos\n",
    "dog.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtiene los hiperónimos más importantes de este synset \n",
    "dog.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('carnivore.n.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowest_common_hypernyms() método utilizado para localizar el hiperónimo más bajo que comparten dos palabras determinadas \n",
    "wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Synset' object has no attribute 'antonyms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Algunas relaciones están definidas por WordNet solo sobre Lemmas\u001b[39;00m\n\u001b[0;32m      2\u001b[0m good \u001b[38;5;241m=\u001b[39m wn\u001b[38;5;241m.\u001b[39msynset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood.a.01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mgood\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantonyms\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Synset' object has no attribute 'antonyms'"
     ]
    }
   ],
   "source": [
    "# Algunas relaciones están definidas por WordNet solo sobre Lemmas\n",
    "good = wn.synset('good.a.01')\n",
    "good.antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cada synset contiene uno o más lemas, que representan un sentido específico de una palabra específica\n",
    "good.lemmas()[0].antonyms()\n",
    "\n",
    "# Nota: Las relaciones que se definen actualmente de esta manera son: antonyms, derivationally_related_forms and pertainyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los antónimos de las palabras. \n",
    "# Antes de agregarlos a una lista, todo lo que tienes que hacer es verificar si cada lemma es un antónimo o no\n",
    "\n",
    "antonyms = []\n",
    "for syn in wn.synsets(\"small\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>synset1.path_similarity(synset2)</b>\n",
    "<ul>\n",
    "    <li>Devuelve un puntaje que indica qué tan similares son los sentidos de dos palabras, según el camino más corto que conecta los sentidos en la taxonomía is-a (hiperónimo / hipónimo).</li>\n",
    "    <li>El puntaje está en el rango de 0 a 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "wn.path_similarity(hit, slap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>synset1.lch_similarity(synset2)</b>\n",
    "<ul>\n",
    "    <li><b>Similitud Leacock-Chodorow:</b> Devuelve una puntuación que denota qué tan parecidos son los sentidos de dos palabras, en función del camino más corto que conecta los sentidos (como anteriormente) y la profundidad máxima de la taxonomía en la que se producen los sentidos.</li>\n",
    "    <li>La relación se da como <code>-log(p/2d)</code> donde <code>p</code> es la longitud de la ruta más corta y <code>d</code> la profundidad de la taxonomía.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.lch_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lch_similarity(hit, slap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando sobre todos los synsets de sustantivos\n",
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "    print(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Comprobar la similaridad entre 20 pares tomados de: <em>A validated translation of the original Miller-Charles (Miller and Charles, 1998) and WordSimilarity-353 (Finkelstein et al., 2001) in Spanish, Romanian, and Arabic. (August 10, 2009)</em>. A partir de la similaridad dada con WordNet en inglés y en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mc = pd.read_csv('CLSR-EK/MC30.csv',delimiter=';',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Parámetros del método <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">Word2Vec</a>:</b></p>\n",
    "\n",
    "Info de más parámetros <a href=\"https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py\">aquí</a>\n",
    "\n",
    "<ul>\n",
    "    <li><em>sentences</em>: iterable de iterables. Una lista de listas de tokens.</li>\n",
    "    <li><em>min_count</em>: int. Ignora todas las palabras con una frecuencia absoluta total menor que esta. - (2, 100)</li>\n",
    "    <li><em>window</em>: int. Distancia máxima entre la palabra actual y la predicha dentro de una oración. P.ej. palabras de ventana a la izquierda y palabras de ventana a la izquierda de nuestro objetivo. - (2, 10)</li>\n",
    "    <li><em>size</em>: int. Dimensionalidad de los vectores de características. - (50, 300)</li>\n",
    "    <li><em>sg</em>: {0, 1}. Algoritmo de entrenamiento: 1 para skip-gram; de lo contrario CBOW.</li>\n",
    "    <li><em>negative</em>: int. Si> 0, se usará un muestreo negativo, el int para negativo especifica cuántas \"palabras irrelevantes\" se deben dibujar (generalmente entre 5-20). Si se establece en 0, no se utiliza ningún muestreo negativo. </li>\n",
    "    <li><em>workers</em>: int. Utilice estos subprocesos de trabajo para entrenar el modelo (= entrenamiento más rápido con máquinas multinúcleo).</li>\n",
    "    <li><em>alpha</em>: float. La tasa de aprendizaje inicial - (0.01, 0.05)</li>\n",
    "    <li><em>max_vocab_size</em>: int. Limita la RAM durante la construcción de vocabulario; si hay más palabras únicas que esta, elimine las poco frecuentes. Cada 10 millones de tipos de palabras necesitan aproximadamente 1 GB de RAM. Establezca None para no tener límite.</li>\n",
    "    <li><em>max_final_vocab</em>: int. Limita el vocabulario a un tamaño de vocabulario de destino seleccionando automáticamente un min_count coincidente. Si el min_count especificado es mayor que el min_count calculado, se utilizará el min_count especificado. Establezca None si no es necesario.</li>\n",
    "    <li><em>sample</em>: float. El umbral para configurar qué palabras de mayor frecuencia se muestrean aleatoriamente, el rango útil es (0, 1e-5).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(temp)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Create CBOW model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mw2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine similarity between \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwonderland\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - CBOW : \u001b[39m\u001b[38;5;124m\"\u001b[39m, model1\u001b[38;5;241m.\u001b[39msimilarity(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwonderland\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "#  Reads ‘alice.txt’ file\n",
    "sample = open(\"alice.txt\", encoding=\"utf8\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "    \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "\n",
    "    data.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = w2v(data, min_count = 1, size = 100, window = 5)\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' and 'wonderland' - CBOW : \", model1.similarity('alice', 'wonderland'))\n",
    "print(\"Cosine similarity between 'alice' and 'machines' - CBOW : \", model1.similarity('alice', 'machines'))\n",
    "\n",
    "# Create Skip Gram model\n",
    "model2 = w2v(data, min_count = 1, size = 100, window = 5, sg = 1)\n",
    "# Print results\n",
    "print(\"Cosine similarity between 'alice' and 'wonderland' - Skip Gram : \", model2.similarity('alice', 'wonderland'))\n",
    "print(\"Cosine similarity between 'alice' and 'machines' - Skip Gram : \", model2.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida indica las similitudes de coseno entre los vectores de palabras \"alicia\", \"país de las maravillas\" y \"máquinas\" para diferentes modelos. Una tarea interesante podría ser cambiar los valores de los parámetros de \"size\" y \"window\" para observar las variaciones en las similitudes del coseno.\n",
    "\n",
    "Puedes oberservar y estudiar otro caso de estudio: \"The Simpsons\", <a href=\"https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\">aquí</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descargando vectores pre-calculados\n",
    "\n",
    "<p>Métodos de generación de word embeddings, los cuales usaremos sus vectores pre-calculados:</p>\n",
    "<ul>\n",
    "    <li><a href=\"https://code.google.com/archive/p/word2vec/\">Word2Vec</a></li>\n",
    "    <li><a href=\"https://fasttext.cc/docs/en/english-vectors.html\">FastText</a></li>\n",
    "    <li><a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path_w2v = \"Word2Vec/GoogleNews-vectors-negative300.bin\"\n",
    "path_ftt = \"fasttext/crawl-300d-2M.vec\"\n",
    "\n",
    "word2v_model = KeyedVectors.load_word2vec_format(path_w2v, binary=True)  # C binary format\n",
    "ftt_model = KeyedVectors.load_word2vec_format(path_ftt, binary=False)  # C text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2v_model.similarity('woman', 'man'))\n",
    "print(ftt_model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "most_similar(positive=None, negative=None, topn=10, clip_start=0, clip_end=None, restrict_vocab=None, indexer=None)\n",
    "\n",
    "    Find the top-N most similar keys. Positive keys contribute positively towards the similarity, negative keys negatively.\n",
    "\n",
    "    This method computes cosine similarity between a simple mean of the projection weight vectors of the given keys and the vectors for each key in the model. The method corresponds to the word-analogy and distance scripts in the original word2vec implementation.\n",
    "\n",
    "    Parameters\n",
    "\n",
    "            positive (list of (str or int or ndarray), optional) – List of keys that contribute positively.\n",
    "\n",
    "            negative (list of (str or int or ndarray), optional) – List of keys that contribute negatively.\n",
    "\n",
    "            topn (int or None, optional) – Number of top-N similar keys to return, when topn is int. When topn is None, then similarities for all keys are returned.\n",
    "\n",
    "            clip_start (int) – Start clipping index.\n",
    "\n",
    "            clip_end (int) – End clipping index.\n",
    "\n",
    "            restrict_vocab (int, optional) – Optional integer which limits the range of vectors which are searched for most-similar values. For example, restrict_vocab=10000 would only check the first 10000 key vectors in the vocabulary order. (This may be meaningful if you’ve sorted the vocabulary by descending frequency.) If specified, overrides any values of clip_start or clip_end.\n",
    "\n",
    "    Returns\n",
    "\n",
    "        When topn is int, a sequence of (key, similarity) is returned. When topn is None, then similarities for all keys are returned as a one-dimensional numpy array with the size of the vocabulary.\n",
    "    Return type\n",
    "\n",
    "        list of (str, float) or numpy.array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
    "print( word2v_model.most_similar(positive=['woman', 'king'],negative=['man']) )\n",
    "\n",
    "print( ftt_model.most_similar(positive=['woman', 'king'],negative=['man'], topn=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qué clave de la lista dada no va con las demás? \n",
    "print( word2v_model.doesnt_match(\"breakfast cereal dinner lunch\".split()) )\n",
    "print( ftt_model.doesnt_match(\"breakfast cereal dinner lunch\".split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access vector for one word\n",
    "vector = word2v_model['computer'] # numpy vector of a word\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia de Word Mover entre dos documentos.\n",
    "\n",
    "Word Mover's Distance (WMD) es una nueva y prometedora herramienta de aprendizaje automático que nos permite enviar una consulta y devolver los documentos más relevantes.\n",
    "\n",
    "WMD nos permite evaluar la \"distancia\" entre dos documentos de manera significativa incluso cuando no tienen palabras en común. Utiliza word2vec incrustaciones vectoriales de palabras. Se ha demostrado que supera a muchos de los métodos más avanzados en la clasificación de k-vecinos más cercanos.\n",
    "\n",
    "WMD se ilustra a continuación para dos oraciones muy similares (ilustración tomada del blog de Vlad Niculae). Las oraciones no tienen palabras en común, pero al hacer coincidir las palabras relevantes, WMD puede medir con precisión la (dis) similitud entre las dos oraciones. El método también utiliza la representación de bolsa de palabras de los documentos (en pocas palabras, las frecuencias de las palabras en los documentos). La intuición detrás del método es que encontramos la “distancia de viaje” mínima entre documentos, en otras palabras, la forma más eficiente de “mover” la distribución del documento 1 a la distribución del documento 2.\n",
    "\n",
    "Este método se introdujo en el artículo \"From Word Embeddings To Document Distances\" de Matt Kusner et al. (http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf). Está inspirado en la \"Distancia del motor de la tierra\" y emplea un solucionador del \"problema del transporte\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Word Mover’s Distance between two documents.\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'\n",
    "\n",
    "sentence_obama = preprocess(sentence_obama)\n",
    "sentence_president = preprocess(sentence_president)\n",
    "\n",
    "similarity_w2v = word2v_model.wmdistance(sentence_obama, sentence_president)\n",
    "similarity_ftt = ftt_model.wmdistance(sentence_obama, sentence_president)\n",
    "\n",
    "similarity_w2v, similarity_ftt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_orange = preprocess('Oranges are my favorite fruit')\n",
    "word2v_model.wmdistance(sentence_obama, sentence_orange), ftt_model.wmdistance(sentence_obama, sentence_orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "Comprobar la similaridad entre 20 pares tomados de: <em>A validated translation of the original Miller-Charles (Miller and Charles, 1998) and WordSimilarity-353 (Finkelstein et al., 2001) in Spanish, Romanian, and Arabic. (August 10, 2009)</em> Probar con los tres vectores mencionados: word2vec, fastTex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mc = pd.read_csv('CLSR-EK/MC30.csv',delimiter=';',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://radimrehurek.com/gensim/models/doc2vec.html\">Doc2Vec</a>\n",
    "\n",
    "## Análisis de sentimientos Movie Reviews con Doc2vec\n",
    "\n",
    "Descarga de corpus Movie reviews, <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">aquí</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos y preparamos el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0\n",
      "Test: 0\n",
      "Terminada cleanText\n",
      "Terminada cleanText\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def read_txt_files(files):\n",
    "    files_list=[]\n",
    "    for i,file_path in enumerate(files):\n",
    "        with open(file_path,'r', encoding='utf8') as infile:\n",
    "            files_list.append(infile.read().encode('utf-8'))\n",
    "    return files_list\n",
    "# limpiar el texto convertir todo a espacios\n",
    "def cleanText(corpus):\n",
    "    corpus = [str(z).replace('<br />', ' ') for z in corpus]\n",
    "    #treat punctuation as individual words\n",
    "    punctuation = '(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\|])'\n",
    "    corpus = [re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub(punctuation, \" \",z)))) for z in corpus]\n",
    "    # corpus = [z.split() for z in corpus]\n",
    "    corpus = [preprocess(z) for z in corpus]\n",
    "    print('Terminada cleanText')\n",
    "    return corpus\n",
    "# textos estan en postivos en negativo \n",
    "#leemos el corpus de entrenamiento\n",
    "train_path_pos='Movie_Review/aclImdb/train/pos/'\n",
    "train_path_neg='Movie_Review/aclImdb/train/neg/'\n",
    "\n",
    "pos_reviews=read_txt_files( glob(train_path_pos+'*.txt') )\n",
    "neg_reviews=read_txt_files( glob(train_path_neg+'*.txt') )\n",
    "\n",
    "x_train=np.concatenate((pos_reviews, neg_reviews))\n",
    "#usamos 1 para sentimiento positivo, 0 para negativo\n",
    "y_train = np.concatenate((np.ones(len(pos_reviews)), np.zeros(len(neg_reviews))))\n",
    "\n",
    "#leemos el corpus de prueba\n",
    "test_path_pos='Movie_Review/aclImdb/test/pos/'\n",
    "test_path_neg='Movie_Review/aclImdb/test/neg/'\n",
    "\n",
    "# carga todo lo que  termine en .txt \n",
    "pos_reviews_test=read_txt_files( glob(test_path_pos+'*.txt') )\n",
    "neg_reviews_test=read_txt_files( glob(test_path_neg+'*.txt') )\n",
    "\n",
    "x_test=np.concatenate((pos_reviews_test, neg_reviews_test))\n",
    "#usamos 1 para sentimiento positivo, 0 para negativo\n",
    "y_test = np.concatenate((np.ones(len(pos_reviews_test)), np.zeros(len(neg_reviews_test))))\n",
    "\n",
    "print ('Train:', len(x_train))\n",
    "print ('Test:', len(x_test))\n",
    "\n",
    "#realizamos un pre-procesamiento básico\n",
    "x_train = cleanText(x_train)\n",
    "x_test = cleanText(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos el Doc2vec con el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import TaggedLineDocument\n",
    "# La implementación de Doc2Vec de Gensim requiere que cada documento / párrafo tenga una etiqueta asociada.\n",
    "# Hacemos esto usando el método TaggedDocument. \n",
    "\n",
    "x_train_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(x_train)]\n",
    "x_test_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(x_test)]\n",
    "\n",
    "vec_size = 300\n",
    "model_dm = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.00025, min_count=1, dm =1)\n",
    "model_dm.build_vocab(x_train_tagged)\n",
    "\n",
    "model_dbow = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.00025, min_count=1, dm =0)\n",
    "model_dbow.build_vocab(x_train_tagged)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('iteration normal {0}'.format(epoch))\n",
    "    model_dm.train(x_train_tagged, total_examples=model_dm.corpus_count, epochs=model_dm.epochs)\n",
    "    # decrease the learning rate\n",
    "    model_dm.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model_dm.min_alpha = model_dm.alpha\n",
    "    \n",
    "    model_dbow.train(x_train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n",
    "    # decrease the learning rate\n",
    "    model_dbow.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtener vectores del conjunto de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model[i]).reshape((1, size)) for i,z in enumerate(corpus)]\n",
    "    return np.concatenate(vecs)\n",
    "\n",
    "#model\n",
    "train_vecs_dm = getVecs(model_dm, x_train, vec_size)\n",
    "train_vecs_dbow = getVecs(model_dbow, x_train, vec_size)\n",
    "train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))\n",
    "\n",
    "test_vecs_dbow = [model_dbow.infer_vector(doc.words, alpha=0.05) for doc in x_test_tagged]\n",
    "test_vecs_dm = [model_dm.infer_vector(doc.words, alpha=0.05) for doc in x_test_tagged]\n",
    "test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))\n",
    "\n",
    "print ('Train shape:', train_vecs.shape)\n",
    "print ('Test shape:', test_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos y evaluamos el clasificador sobre los vectores obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_vecs, y_train)\n",
    "print ('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construimos vector de documentos a partir de vectores de palabras: Word2Vec y fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(text, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "n_dim = 300\n",
    "\n",
    "train_vecs = np.concatenate([buildWordVector(z, n_dim, word2v_model) for z in x_train])\n",
    "test_vecs = np.concatenate([buildWordVector(z, n_dim, word2v_model) for z in x_test])\n",
    "\n",
    "train_vecs = scale(train_vecs)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_vecs, y_train)\n",
    "print ('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "n_dim = 300\n",
    "\n",
    "train_vecs = np.concatenate([buildWordVector(z, n_dim, ftt_model) for z in x_train])\n",
    "test_vecs = np.concatenate([buildWordVector(z, n_dim, ftt_model) for z in x_test])\n",
    "\n",
    "train_vecs = scale(train_vecs)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_vecs, y_train)\n",
    "print ('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de mglearn en Anaconda\n",
    "<code>conda install pip</code>\n",
    "<code>pip install mglearn</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Librerías que nos ayuda para graficar las datas\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set de datos de cancer, son estudios de 30 características para decir si un tumor es benigno o maligno\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# En el editor de Jupyter nos permite ver las gráficas dentro del editor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qués hace PCA?\n",
    "mglearn.plots.plot_pca_illustration()\n",
    "\n",
    "# 1. Encuentre el eje para el componente 1 y el 2 (el C1 es el eje de mayor varianza y el C2 es ortogonal al eje del C1)\n",
    "# 2. Rota la data horizontal, restando el promedio a cada dato, centrando el set de datos en el eje 0\n",
    "# 3. Grafica una línea recta, donde el eje horizontal es el componente 1 y el 2do eje serían las características de la gráfica 1\n",
    "# 4. Le suma al promdio restado para hacer la rotación.\n",
    "# Concluyendo con una gráfica que tiene la misma dirección a la data original, la diferencia es que no hay tanto ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "cancer = load_breast_cancer()\n",
    "# Mostrar los nombres de las características\n",
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.feature_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # 2 componentes = 2 dimensiones (mediciones vs características)\n",
    "pca.fit(cancer.data) # Para entrenar el PCA con los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para transformar los datos\n",
    "transformada = pca.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.data.shape)\n",
    "print(transformada.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(transformada[:,0],transformada[:,1], cancer.target)\n",
    "plt.legend(cancer.target_names,loc='best')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede visualizar una abstracción de los datos. En el que se refleja un comportamiento en la información. Todos los tumores benignos se agrupan a la izquierda, y gran parte de los tumores malignos al lado derecho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las mediciones de los datos, se transforman a una escala similar en el rango 0-1.\n",
    "# Ayuda a no estar manejando valores muy pequeños contra valores muy grandes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "escala = MinMaxScaler()\n",
    "escala.fit(cancer.data)\n",
    "escalada = escala.transform(cancer.data)\n",
    "\n",
    "pca.fit(escalada)\n",
    "transformada = pca.transform(escalada)\n",
    "\n",
    "mglearn.discrete_scatter(transformada[:,0],transformada[:,1], cancer.target)\n",
    "plt.legend(cancer.target_names,loc='best')\n",
    "plt.gca()\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la misma data, pero escalada en rangos menores. Y se pueden vizualizar mejor las correlaciones antes observadas. En este sentido se puede decir que esta data si nos sirve para hacer predicciones; y que si a un algoritmo lo alimentamos con la data original o con la data ya transformada con PCA, en el caso del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.data # Ver la data no escalada, rangos de diferentes escalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escalada # Ver la data escalada los valores en rango 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso Cluster de palabras\n",
    "\n",
    "Descarga de lista de palabras, <a href=\"https://www.enchantedlearning.com/wordlist/\">aquí</a>\n",
    "<ul>\n",
    "    <li>Food</li>\n",
    "    <li>Sports</li>\n",
    "    <li>Weather</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words/food_words.txt', 'r') as infile:\n",
    "    food_words = infile.readlines()\n",
    "with open('words/sports_words.txt', 'r') as infile:\n",
    "    sports_words = infile.readlines()\n",
    "with open('words/weather_words.txt', 'r') as infile:\n",
    "    weather_words = infile.readlines()\n",
    "    \n",
    "def getWordVecs(words, model):\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            vecs.append(model[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vecs = np.concatenate(vecs)\n",
    "    return vecs\n",
    "\n",
    "food_w2v = getWordVecs(food_words, word2v_model)\n",
    "sports_w2v = getWordVecs(sports_words, word2v_model)\n",
    "weather_w2v = getWordVecs(weather_words, word2v_model)\n",
    "\n",
    "food_ftt = getWordVecs(food_words, ftt_model)\n",
    "sports_ftt = getWordVecs(sports_words, ftt_model)\n",
    "weather_ftt = getWordVecs(weather_words, ftt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de clusters\n",
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame( np.concatenate((food_w2v, sports_w2v, weather_w2v)) )\n",
    "\n",
    "# normalizamos los datos\n",
    "x = StandardScaler().fit_transform(df)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(\n",
    "    data = principalComponents,\n",
    "    columns = ['principal component 1', 'principal component 2']\n",
    ")\n",
    "target =  DataFrame(\n",
    "    data = ['food']*len(food_w2v) + ['sports']*len(sports_w2v) + ['weather']*len(weather_w2v),\n",
    "    columns = ['target']\n",
    ")\n",
    "finalDf = pd.concat([principalDf, target], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['food', 'sports', 'weather']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(\n",
    "        finalDf.loc[indicesToKeep, 'principal component 1'],\n",
    "        finalDf.loc[indicesToKeep, 'principal component 2'],\n",
    "        c = color,\n",
    "        s = 50\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de clusters\n",
    "#### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame( np.concatenate((food_ftt, sports_ftt, weather_ftt)) )\n",
    "\n",
    "# normalizamos los datos\n",
    "x = StandardScaler().fit_transform(df)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(\n",
    "    data = principalComponents,\n",
    "    columns = ['principal component 1', 'principal component 2']\n",
    ")\n",
    "target =  DataFrame(\n",
    "    data = ['food']*len(food_ftt) + ['sports']*len(sports_ftt) + ['weather']*len(weather_ftt),\n",
    "    columns = ['target']\n",
    ")\n",
    "finalDf = pd.concat([principalDf, target], axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['food', 'sports', 'weather']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(\n",
    "        finalDf.loc[indicesToKeep, 'principal component 1'],\n",
    "        finalDf.loc[indicesToKeep, 'principal component 2'],\n",
    "        c = color,\n",
    "        s = 50\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar Caso de Estudio: alquiler de vivienda\n",
    "\n",
    "<p>Imaginemos que queremos predecir los precios de alquiler de vivienda del mercado. Al recopilar información de diversas fuentes tendremos en cuenta variables como tipo de vivienda, tamaño de vivienda, antigüedad, servicios, habitaciones, con/sin jardín, con/sin piscina, con/sin muebles  pero también podemos tener en cuenta la distancia al centro, si hay colegio en las cercanías, o supermercados, si es un entorno ruidoso, si tiene autopistas en las cercanías, la “seguridad del barrio”, si se aceptan mascotas, tiene wifi, tiene garaje, trastero… y seguir y seguir sumando variables.</p>\n",
    "<p>Utilizaremos un archivo csv de entrada, en el cual decidíamos si convenía alquilar o comprar casa dadas 9 dimensiones. En este ejemplo:</p>\n",
    "<ul>\n",
    "    <li>normalizamos los datos de entrada,</li>\n",
    "    <li>aplicamos PCA</li>\n",
    "    <li>y veremos que con 5 de las nuevas dimensiones (y descartando 4) obtendremos</li>\n",
    "    <ul>\n",
    "        <li>hasta un 85% de variación y</li>\n",
    "        <li>buenas predicciones.</li>\n",
    "    </ul>\n",
    "    <li>Realizaremos 2 gráficas:</li>\n",
    "    <ul>\n",
    "        <li>una con el acumulado de variabilidad explicada y</li>\n",
    "        <li>una gráfica 2D, en donde el eje X e Y serán los 2 primero componentes principales obtenidos por PCA.</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<p>Y veremos cómo los resultados “comprar ó alquilar” tienen bastante buena separación en 2 dimensiones.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicios tomado de https://www.aprendemachinelearning.com/comprende-principal-component-analysis/\n",
    "\n",
    "# importamos librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# cargamos los datos de entrada\n",
    "dataframe = pd.read_csv(r\"comprar_alquilar.csv\")\n",
    "print(dataframe.tail(10))\n",
    " \n",
    "# normalizamos los datos\n",
    "scaler = StandardScaler()\n",
    "df = dataframe.drop(['comprar'], axis=1) # quito la variable dependiente \"Y\"\n",
    "scaler.fit(df) # calculo la media para poder hacer la transformacion\n",
    "X_scaled = scaler.transform(df)# Ahora si, escalo los datos y los normalizo\n",
    " \n",
    "# Instanciamos objeto PCA y aplicamos\n",
    "pca=PCA(n_components=9) # Otra opción es instanciar pca sólo con dimensiones nuevas hasta obtener un mínimo\n",
    "pca.fit(X_scaled) # obtener los componentes principales\n",
    "X_pca=pca.transform(X_scaled) # convertimos nuestros datos con las nuevas dimensiones de PCA\n",
    " \n",
    "print(\"shape of X_pca\", X_pca.shape)\n",
    "expl = pca.explained_variance_ratio_\n",
    "print(expl)\n",
    "print('suma:',sum(expl[0:5]))\n",
    "#Vemos que con 5 componentes tenemos algo mas del 85% de varianza explicada\n",
    " \n",
    "#graficamos el acumulado de varianza explicada en las nuevas dimensiones\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    " \n",
    "#graficamos en 2 Dimensiones, tomando los 2 primeros componentes principales\n",
    "Xax=X_pca[:,0]\n",
    "Yax=X_pca[:,1]\n",
    "labels=dataframe['comprar'].values\n",
    "cdict={0:'red',1:'green'}\n",
    "labl={0:'Alquilar',1:'Comprar'}\n",
    "marker={0:'*',1:'o'}\n",
    "alpha={0:.3, 1:.5}\n",
    "fig,ax=plt.subplots(figsize=(7,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "for l in np.unique(labels):\n",
    "    ix=np.where(labels==l)\n",
    "    ax.scatter(Xax[ix],Yax[ix],c=cdict[l],label=labl[l],s=40,marker=marker[l],alpha=alpha[l])\n",
    " \n",
    "plt.xlabel(\"First Principal Component\",fontsize=14)\n",
    "plt.ylabel(\"Second Principal Component\",fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
